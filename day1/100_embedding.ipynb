{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c387e7ef",
   "metadata": {},
   "source": [
    "#  문장 내의 단어들을 임베딩\n",
    "- keras.layers.Embedding 레이어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db507fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# 샘플 데이터: 간단한 문장들의 모음\n",
    "sentences = [\n",
    "    \"I love machine learning\",\n",
    "    \"I love coding in Python\",\n",
    "    \"Deep learning is fun\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dc90226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 1,\n",
       " 'love': 2,\n",
       " 'machine': 3,\n",
       " 'learning': 4,\n",
       " 'coding': 5,\n",
       " 'in': 6,\n",
       " 'Python': 7,\n",
       " 'Deep': 8,\n",
       " 'is': 9,\n",
       " 'fun': 10}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 문장을 단어로 분할하고, 각 단어에 대한 고유한 인덱스를 생성\n",
    "word_index = {}\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        if word not in word_index:\n",
    "            word_index[word] = len(word_index) + 1\n",
    "            \n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5031225f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'love', 'machine', 'learning'],\n",
       " ['I', 'love', 'coding', 'in', 'Python'],\n",
       " ['Deep', 'learning', 'is', 'fun']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장들을 단어 별로 쪼개서 확인\n",
    "sequences = [[word for word in sentence.split()] for sentence in sentences]\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98af754a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [1, 2, 5, 6, 7], [8, 4, 9, 10]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장들을 단어 인덱스의 시퀀스로 변환\n",
    "sequences = [[word_index[word] for word in sentence.split()] for sentence in sentences]\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51c37c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  0],\n",
       "       [ 1,  2,  5,  6,  7],\n",
       "       [ 8,  4,  9, 10,  0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장들 중 가장 긴 것의 길이를 구함\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "\n",
    "# 모든 문장을 가장 긴 문장의 길이로 패딩\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bddcd17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 8)\n",
      "tf.Tensor(\n",
      "[[[ 0.03909704 -0.01060718  0.04862044  0.03201996  0.02909395\n",
      "   -0.00947579 -0.02956091 -0.03168044]\n",
      "  [-0.013817   -0.00240855  0.00238355 -0.04222057  0.01026149\n",
      "    0.0435562   0.0378672  -0.03447417]\n",
      "  [ 0.03585944  0.00039071 -0.04533761 -0.00523996 -0.04810028\n",
      "    0.02324573  0.03456834 -0.01094872]\n",
      "  [ 0.01764271  0.04724101  0.01234831  0.0374014  -0.02788237\n",
      "   -0.04083592 -0.03945457 -0.02182654]\n",
      "  [ 0.04623378 -0.03750336  0.04909052 -0.03081771  0.01201757\n",
      "   -0.02501727 -0.01514336 -0.01846459]]\n",
      "\n",
      " [[ 0.03909704 -0.01060718  0.04862044  0.03201996  0.02909395\n",
      "   -0.00947579 -0.02956091 -0.03168044]\n",
      "  [-0.013817   -0.00240855  0.00238355 -0.04222057  0.01026149\n",
      "    0.0435562   0.0378672  -0.03447417]\n",
      "  [-0.0302022  -0.01080042  0.02130855 -0.0451139   0.04262223\n",
      "    0.03640341 -0.02059165 -0.02824067]\n",
      "  [ 0.00986376  0.0304972  -0.00660665 -0.04106548 -0.04125528\n",
      "    0.03942207 -0.04930611  0.04627163]\n",
      "  [-0.00892039 -0.00493193  0.03103844 -0.03783051  0.03946117\n",
      "   -0.00293168  0.04447493  0.03779266]]\n",
      "\n",
      " [[ 0.00946379  0.03786914 -0.00239009  0.04606186  0.00341497\n",
      "    0.04699056 -0.0236748  -0.02775272]\n",
      "  [ 0.01764271  0.04724101  0.01234831  0.0374014  -0.02788237\n",
      "   -0.04083592 -0.03945457 -0.02182654]\n",
      "  [ 0.03048969  0.00952087 -0.00924629  0.01259798  0.00988365\n",
      "    0.04364509 -0.04392257 -0.00149893]\n",
      "  [-0.04264003 -0.04836743  0.00083735 -0.01394806  0.03966172\n",
      "   -0.03742082  0.04712312 -0.02511871]\n",
      "  [ 0.04623378 -0.03750336  0.04909052 -0.03081771  0.01201757\n",
      "   -0.02501727 -0.01514336 -0.01846459]]], shape=(3, 5, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Embedding 레이어 생성\n",
    "embedding_dim = 8 #단어에 부여하는 특성의 수. 특성의 라벨은 우리가 정하는게 아니라서 개별 특성이 뭔지 모른다. 그래서 블랙박스 모델이다.\n",
    "embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_length)\n",
    "\n",
    "# 패딩된 시퀀스를 Embedding 레이어에 통과시켜 임베딩된 결과를 얻음\n",
    "embedded_sequences = embedding_layer(padded_sequences)\n",
    "\n",
    "print(embedded_sequences.shape)\n",
    "print(embedded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd0d6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential()  # Note the 'keras' module is part of TensorFlow\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim))  # You need to specify the input_dim and output_dim\n",
    "model.add(Dense(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "691a1486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Layer Shape : (11, 8)\n",
      "Embedding Layer Weights (Word Embeddings):\n",
      " [[-0.02520714  0.03637946 -0.04417964  0.01727331 -0.02484065 -0.03742831\n",
      "   0.04018419 -0.01577323]\n",
      " [ 0.01964789  0.04982078 -0.00259792  0.01207108  0.00501344 -0.03558845\n",
      "   0.01545152  0.01110258]\n",
      " [ 0.03608132 -0.02834145  0.01184057  0.03801778 -0.03405376 -0.03874751\n",
      "   0.02867026 -0.03788869]\n",
      " [-0.02202942  0.03358087  0.0335927   0.03488762  0.00454522  0.04015798\n",
      "   0.03426404 -0.01249205]\n",
      " [ 0.03454043  0.02845288  0.03782107 -0.04364526  0.03439027 -0.00610853\n",
      "  -0.02683942 -0.01086602]\n",
      " [-0.03343445 -0.00361217 -0.01967759 -0.00299609 -0.03495841  0.02485074\n",
      "  -0.01322914 -0.02464907]\n",
      " [ 0.00668986  0.00375447  0.02734328  0.00965524  0.00485077 -0.02006277\n",
      "  -0.04011371 -0.00481031]\n",
      " [ 0.03586033 -0.03422767  0.04377914  0.02209132  0.00246817  0.04877276\n",
      "   0.02276179  0.00087597]\n",
      " [-0.01222767  0.04570584 -0.01642164 -0.03781677 -0.00629463 -0.01897699\n",
      "   0.01359895 -0.03158937]\n",
      " [ 0.04962182 -0.01306261 -0.04645792 -0.04235701  0.01228185 -0.00997714\n",
      "  -0.01391023  0.03331753]\n",
      " [ 0.00954638 -0.03247216  0.01955881  0.02164834  0.04768011  0.03900541\n",
      "   0.00110491 -0.01345045]]\n",
      "\n",
      "\n",
      "Embedding for 'love':\n",
      " [ 0.03608132 -0.02834145  0.01184057  0.03801778 -0.03405376 -0.03874751\n",
      "  0.02867026 -0.03788869]\n"
     ]
    }
   ],
   "source": [
    "# Embedding 레이어의 가중치 (단어 임베딩 행렬) 출력\n",
    "embeddings = embedding_layer.get_weights()[0]\n",
    "print(\"Embedding Layer Shape :\", embeddings.shape)\n",
    "print(\"Embedding Layer Weights (Word Embeddings):\\n\", embeddings)\n",
    "print()\n",
    "\n",
    "# 예: 'love'라는 단어의 임베딩 벡터를 출력\n",
    "print(\"\\nEmbedding for 'love':\\n\", embeddings[word_index['love']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf45463",
   "metadata": {},
   "source": [
    "0은 보통 패딩을 나타내는 인덱스로 사용됩니다. 결과적으로, Embedding 레이어의 가중치 행렬의 크기는 (고유한 단어 수 + 1, 임베딩 벡터의 차원수)가 되므로, (11, 8)이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0718544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
